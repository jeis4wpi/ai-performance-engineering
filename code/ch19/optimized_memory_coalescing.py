"""optimized_memory_coalescing.py - Optimized memory access with coalescing. Demonstrates coalesced memory access patterns in adaptive memory management. Implements Benchmark protocol for harness integration. """ from __future__ import annotations import sys from pathlib import Path repo_root = Path(__file__).parent.parent if str(repo_root) not in sys.path: sys.path.insert(0, str(repo_root)) import torch import torch.nn as nn from typing import Optional from common.python.benchmark_harness import ( Benchmark, BenchmarkConfig, BenchmarkHarness, BenchmarkMode ) def resolve_device() -> torch.device: """Return CUDA device if available.""" if not torch.cuda.is_available(): raise RuntimeError("CUDA required for ch19") return torch.device("cuda") class OptimizedMemoryCoalescingBenchmark(Benchmark): """Optimized: coalesced memory access.""" def __init__(self): self.device = resolve_device() self.model = None self.x = None self.batch_size = 4 self.seq_len = 1024 self.hidden_dim = 1024 def setup(self) -> None: """Setup: Initialize model with coalesced access pattern.""" # Simple transformer layer self.model = nn.Sequential( nn.Linear(self.hidden_dim, self.hidden_dim * 4), nn.ReLU(), nn.Linear(self.hidden_dim * 4, self.hidden_dim), ) self.model = self.model.to(self.device).to(dtype=torch.bfloat16).eval() # Optimization: Ensure contiguous memory layout for coalesced access # Contiguous memory allows threads in a warp to access consecutive memory locations self.x = torch.randn(self.batch_size, self.seq_len, self.hidden_dim, device=self.device, dtype=torch.bfloat16).contiguous() def benchmark_fn(self) -> None: """Benchmark: Coalesced memory access.""" torch.cuda.nvtx.range_push("optimized_memory_coalescing") try: with torch.no_grad(): # Optimization: Contiguous memory layout enables coalesced access # Threads in a warp access consecutive memory locations # This maximizes memory bandwidth utilization _ = self.model(self.x) finally: torch.cuda.nvtx.range_pop() def teardown(self) -> None: """Cleanup.""" del self.model, self.x if torch.cuda.is_available(): torch.cuda.empty_cache() def get_config(self) -> BenchmarkConfig: """Return benchmark-specific config.""" return BenchmarkConfig( iterations=20, warmup=5, ) def validate_result(self) -> Optional[str]: """Validate benchmark result.""" if self.model is None: return "Model not initialized" if self.x is None: return "Input tensor not initialized" try: with torch.no_grad(): output = self.model(self.x) if output.shape != (self.batch_size, self.seq_len, self.hidden_dim): return f"Output shape mismatch" except Exception as e: return f"Model forward pass failed: {e}" return None def get_benchmark() -> Benchmark: """Factory function for harness discovery.""" return OptimizedMemoryCoalescingBenchmark() def main() -> None: """Standalone execution.""" harness = BenchmarkHarness( mode=BenchmarkMode.CUSTOM, config=BenchmarkConfig(iterations=20, warmup=5) ) benchmark = OptimizedMemoryCoalescingBenchmark() result = harness.benchmark(benchmark) print("=" * 70) print("Optimized: Memory Coalescing (Coalesced)") print("=" * 70) print(f"Average time: {result.mean_ms:.3f} ms") print(" Tip: Contiguous memory layout enables coalesced access, maximizing bandwidth") if __name__ == "__main__": main() 