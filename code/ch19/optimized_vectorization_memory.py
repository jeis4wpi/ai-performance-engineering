"""optimized_vectorization_memory.py - Optimized memory management with vectorization. Demonstrates memory operations optimized with vectorization (SIMD). Vectorization processes multiple elements per instruction for better efficiency. Implements Benchmark protocol for harness integration. """ from __future__ import annotations import sys from pathlib import Path repo_root = Path(__file__).parent.parent if str(repo_root) not in sys.path: sys.path.insert(0, str(repo_root)) import torch from typing import Optional from common.python.benchmark_harness import ( Benchmark, BenchmarkConfig, ) def resolve_device() -> torch.device: """Return CUDA device if available.""" if not torch.cuda.is_available(): raise RuntimeError("CUDA required for ch19") return torch.device("cuda") class OptimizedVectorizationMemoryBenchmark(Benchmark): """Optimized: Memory operations with vectorization.""" def __init__(self): self.device = resolve_device() self.input = None self.output = None self.N = 1_000_000 def setup(self) -> None: """Setup: Initialize tensors for vectorized operations.""" torch.manual_seed(42) # Optimization: Vectorized operations # Vectorization uses SIMD instructions to process multiple elements per instruction # Improves memory bandwidth utilization and reduces instruction overhead # PyTorch operations automatically use vectorization when beneficial self.input = torch.randn(self.N, device=self.device, dtype=torch.float32) self.output = torch.empty(self.N, device=self.device, dtype=torch.float32) torch.cuda.synchronize() def benchmark_fn(self) -> None: """Benchmark: Vectorized memory operations.""" torch.cuda.nvtx.range_push("optimized_vectorization_memory") try: # Optimization: Vectorized operations # Vectorization uses SIMD instructions to process multiple elements # PyTorch operations automatically leverage vectorization # Improves memory bandwidth utilization through parallel element processing self.output = self.input * 2.0 + 1.0 # Vectorized operations process multiple elements per instruction # Improves memory efficiency compared to scalar operations # See ch5 for full vectorization optimization techniques finally: torch.cuda.nvtx.range_pop() def teardown(self) -> None: """Teardown: Clean up resources.""" self.input = None self.output = None torch.cuda.empty_cache() def get_config(self) -> BenchmarkConfig: """Return benchmark configuration.""" return BenchmarkConfig( iterations=100, warmup=10, ) def validate_result(self) -> Optional[str]: """Validate benchmark result.""" if self.output is None: return "Output tensor not initialized" return None def get_benchmark() -> Benchmark: """Factory function for benchmark discovery.""" return OptimizedVectorizationMemoryBenchmark() if __name__ == '__main__': from common.python.benchmark_harness import BenchmarkHarness, BenchmarkMode benchmark = get_benchmark() harness = BenchmarkHarness( mode=BenchmarkMode.CUSTOM, config=benchmark.get_config() ) result = harness.benchmark(benchmark) print(f"\nOptimized Vectorization Memory: {result.mean_ms:.3f} ms") print(" Tip: Vectorization processes multiple elements per instruction for better memory efficiency") 