// graph_bandwidth_kernels.cu - CUDA kernels for graph bandwidth benchmarks
// Can be loaded as PyTorch CUDA extension

#include <torch/extension.h>
#include <cuda_runtime.h>
#include "profiling_helpers.cuh"

// Simple memory copy kernel
__global__ void copy_kernel(float* dst, const float* src, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < n) {
        dst[idx] = src[idx];
    }
}

namespace {

#define CHECK_CUDA(call)                                                       \
    do {                                                                       \
        cudaError_t _status = (call);                                          \
        TORCH_CHECK(_status == cudaSuccess,                                    \
                    "CUDA error at ", __FILE__, ":", __LINE__, " - ",          \
                    cudaGetErrorString(_status));                              \
    } while (0)

} // anonymous namespace

void separate_kernel_launches(torch::Tensor dst, torch::Tensor src, int iterations) {
    TORCH_CHECK(dst.is_cuda(), "dst must be CUDA tensor");
    TORCH_CHECK(src.is_cuda(), "src must be CUDA tensor");
    TORCH_CHECK(dst.dtype() == torch::kFloat32, "dst must be float32");
    TORCH_CHECK(src.dtype() == torch::kFloat32, "src must be float32");
    
    // Ensure we're on the correct device
    int device_id = dst.device().index();
    CHECK_CUDA(cudaSetDevice(device_id));
    
    int n = src.size(0);
    int threads_per_block = 256;
    int num_blocks = (n + threads_per_block - 1) / threads_per_block;
    
    // Use default stream (nullptr) - this is the legacy default stream
    // PyTorch operations on default stream will be properly synchronized
    cudaStream_t stream = nullptr;
    
    {
        PROFILE_KERNEL_LAUNCH("separate_kernel_launches");
        for (int i = 0; i < iterations; ++i) {
            copy_kernel<<<num_blocks, threads_per_block, 0, stream>>>(
                dst.data_ptr<float>(),
                src.data_ptr<float>(),
                n
            );
        }
        CHECK_CUDA(cudaGetLastError());
    }
    // Note: No explicit synchronization here - PyTorch benchmark harness handles synchronization
}

void graph_kernel(torch::Tensor dst, torch::Tensor src, int iterations) {
    TORCH_CHECK(dst.is_cuda(), "dst must be CUDA tensor");
    TORCH_CHECK(src.is_cuda(), "src must be CUDA tensor");
    TORCH_CHECK(dst.dtype() == torch::kFloat32, "dst must be float32");
    TORCH_CHECK(src.dtype() == torch::kFloat32, "src must be float32");
    
    int n = src.size(0);
    int threads_per_block = 256;
    int num_blocks = (n + threads_per_block - 1) / threads_per_block;
    
    // Create a non-blocking stream for graph capture (required for CUDA graphs)
    // Use the same device as the tensors for consistency
    int device_id = dst.device().index();
    cudaStream_t stream = nullptr;
    CHECK_CUDA(cudaSetDevice(device_id));
    CHECK_CUDA(cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking));
    
    cudaGraph_t graph = nullptr;
    cudaGraphExec_t exec = nullptr;
    
    try {
        CHECK_CUDA(cudaStreamBeginCapture(stream, cudaStreamCaptureModeGlobal));
        copy_kernel<<<num_blocks, threads_per_block, 0, stream>>>(
            dst.data_ptr<float>(),
            src.data_ptr<float>(),
            n
        );
        CHECK_CUDA(cudaGetLastError());
        CHECK_CUDA(cudaStreamEndCapture(stream, &graph));
        CHECK_CUDA(cudaGraphInstantiate(&exec, graph, nullptr, nullptr, 0));
        
        {
            PROFILE_KERNEL_LAUNCH("graph_kernel");
            for (int i = 0; i < iterations; ++i) {
                CHECK_CUDA(cudaGraphLaunch(exec, stream));
            }
        }
        
        // Synchronize only the stream we're using (not the entire device)
        // Required here because we created our own stream for graph capture
        CHECK_CUDA(cudaStreamSynchronize(stream));
    } catch (...) {
        if (exec != nullptr) {
            cudaGraphExecDestroy(exec);
        }
        if (graph != nullptr) {
            cudaGraphDestroy(graph);
        }
        if (stream != nullptr) {
            cudaStreamDestroy(stream);
        }
        throw;
    }
    
    cudaGraphExecDestroy(exec);
    cudaGraphDestroy(graph);
    cudaStreamDestroy(stream);
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def("separate_kernel_launches", &separate_kernel_launches, "Separate kernel launches (baseline)");
    m.def("graph_kernel", &graph_kernel, "CUDA graph kernel (optimized)");
}
